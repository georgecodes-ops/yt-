#!/usr/bin/env python3
"""
Demo script showing the complete content flow:
Research Data -> Ollama/Mistral -> WAN Video Generation

This demonstrates how the system is structured to feed research data
to an LLM (Ollama/Mistral) and then to WAN for generating quality content.
"""

import asyncio
import logging
from pathlib import Path
import sys

# Add src to path
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

from content.content_intelligence import ContentIntelligenceAggregator
from content.enhanced_content_pipeline import EnhancedContentPipeline
from content.universal_content_engine import UniversalContentEngine
from content.ollama_client import OllamaClient
from utils.smart_model_manager import SmartModelManager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def demonstrate_content_flow():
    """Demonstrate the complete content generation flow"""
    
    logger.info("ğŸš€ Starting Content Flow Demonstration")
    logger.info("=" * 50)
    
    # Step 1: Initialize components
    logger.info("ğŸ“‹ Step 1: Initializing system components...")
    
    try:
        # Initialize intelligence aggregator
        intelligence = ContentIntelligenceAggregator()
        logger.info("âœ… Content Intelligence Aggregator initialized")
        
        # Initialize Ollama client
        ollama_client = OllamaClient()
        logger.info("âœ… Ollama client initialized")
        
        # Check Ollama health
        ollama_healthy = ollama_client.check_health()
        logger.info(f"ğŸ” Ollama service health: {'âœ… Available' if ollama_healthy else 'âŒ Unavailable'}")
        
        # Initialize content pipeline
        content_pipeline = EnhancedContentPipeline()
        logger.info("âœ… Enhanced Content Pipeline initialized")
        
        # Initialize universal content engine with AI
        content_engine = UniversalContentEngine(use_ai=ollama_healthy)
        logger.info(f"âœ… Universal Content Engine initialized (AI: {'Enabled' if ollama_healthy else 'Disabled'})")
        
        # Initialize model manager
        model_manager = SmartModelManager()
        logger.info("âœ… Smart Model Manager initialized")
        
    except Exception as e:
        logger.error(f"âŒ Component initialization failed: {e}")
        return
    
    logger.info("\n" + "=" * 50)
    
    # Step 2: Generate research-driven content ideas
    logger.info("ğŸ“Š Step 2: Generating research-driven content ideas...")
    
    try:
        # Generate content ideas from research data
        content_ideas = await content_pipeline.generate_intelligence_driven_content(count=3)
        
        if content_ideas:
            logger.info(f"âœ… Generated {len(content_ideas)} content ideas")
            
            for i, idea in enumerate(content_ideas, 1):
                idea_data = idea.get('idea_source', {})
                logger.info(f"\nğŸ’¡ Idea {i}:")
                logger.info(f"   Title: {idea_data.get('title', 'N/A')}")
                logger.info(f"   Type: {idea_data.get('type', 'N/A')}")
                logger.info(f"   Uniqueness Score: {idea_data.get('uniqueness_score', 'N/A')}")
                logger.info(f"   AI Generated: {idea.get('content', {}).get('ai_generated', False)}")
                
                # Show content structure
                content = idea.get('content', {}).get('content', {})
                if content:
                    logger.info(f"   Hook: {content.get('hook', 'N/A')[:100]}...")
                    logger.info(f"   Main Content: {content.get('main_content', 'N/A')[:100]}...")
        else:
            logger.warning("âš ï¸ No content ideas generated")
            
    except Exception as e:
        logger.error(f"âŒ Content idea generation failed: {e}")
    
    logger.info("\n" + "=" * 50)
    
    # Step 3: Demonstrate direct Ollama integration
    logger.info("ğŸ¤– Step 3: Testing direct Ollama integration...")
    
    if ollama_healthy:
        try:
            # Sample research data
            sample_research = {
                'news_items': ['Bitcoin reaches new ATH', 'Fed announces rate decision', 'Tech stocks surge'],
                'trends': ['DeFi adoption', 'AI investment', 'Green energy'],
                'academic_insights': ['Market efficiency study', 'Behavioral finance research']
            }
            
            # Generate AI content directly
            ai_script = await ollama_client.generate_video_script(
                topic="Bitcoin Investment Strategy",
                research_data=sample_research,
                target_audience="young_investors",
                tone="engaging"
            )
            
            if ai_script.get('status') == 'success':
                logger.info("âœ… Direct Ollama generation successful")
                logger.info(f"   Research Context: {ai_script.get('research_context', 'N/A')[:100]}...")
                
                content = ai_script.get('content', {})
                if isinstance(content, dict):
                    logger.info(f"   Generated Hook: {content.get('hook', 'N/A')[:100]}...")
                    logger.info(f"   Generated CTA: {content.get('call_to_action', 'N/A')[:100]}...")
            else:
                logger.warning(f"âš ï¸ Ollama generation failed: {ai_script.get('error', 'Unknown error')}")
                
        except Exception as e:
            logger.error(f"âŒ Direct Ollama test failed: {e}")
    else:
        logger.info("âš ï¸ Skipping Ollama test - service not available")
    
    logger.info("\n" + "=" * 50)
    
    # Step 4: Show model manager status
    logger.info("ğŸ”§ Step 4: Checking model availability...")
    
    try:
        model_health = model_manager.get_model_health_summary()
        logger.info("ğŸ“Š Model Health Summary:")
        for model, status in model_health.items():
            logger.info(f"   {model}: {'âœ… Available' if status else 'âŒ Unavailable'}")
        
        optimal_model = model_manager.get_optimal_model()
        logger.info(f"ğŸ¯ Optimal Model: {optimal_model}")
        
    except Exception as e:
        logger.error(f"âŒ Model status check failed: {e}")
    
    logger.info("\n" + "=" * 50)
    logger.info("ğŸ‰ Content Flow Demonstration Complete!")
    
    # Summary
    logger.info("\nğŸ“‹ SUMMARY:")
    logger.info("1. âœ… Research data is aggregated from multiple sources")
    logger.info("2. âœ… Content ideas are generated with research context")
    logger.info(f"3. {'âœ…' if ollama_healthy else 'âš ï¸'} Ollama/Mistral processes research data into scripts")
    logger.info("4. âœ… WAN video generators use AI-enhanced scripts")
    logger.info("5. âœ… System gracefully falls back to templates if AI unavailable")
    
    if ollama_healthy:
        logger.info("\nğŸš€ SYSTEM STATUS: Fully operational with AI content generation!")
    else:
        logger.info("\nâš ï¸ SYSTEM STATUS: Operational with template fallback (Ollama unavailable)")

if __name__ == "__main__":
    asyncio.run(demonstrate_content_flow())